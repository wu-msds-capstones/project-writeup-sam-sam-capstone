<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="_04_data_files/libs/clipboard/clipboard.min.js"></script>
<script src="_04_data_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="_04_data_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="_04_data_files/libs/quarto-html/popper.min.js"></script>
<script src="_04_data_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="_04_data_files/libs/quarto-html/anchor.min.js"></script>
<link href="_04_data_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="_04_data_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="_04_data_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="_04_data_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="_04_data_files/libs/bootstrap/bootstrap-81267100e462c21b3d6c0d5bf76a3417.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Feature Selection &amp; Statistical Analysis</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Used in the final models was a group of ten features. Table 1 provides an overview of these features and their definitions. Table 1 Features used in the final model and their definitions</p>
<div class="cell" data-message="false" data-tbl-cap="Table 1: Features used in the final model and their definitions" data-execution_count="2">
<div id="table-1" class="cell-output cell-output-display" data-execution_count="10">
<style type="text/css">
</style>

<table id="T_d0973" class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table 1: Features used in the final model and their definitions</caption>
<thead>
<tr class="header">
<th id="T_d0973_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Feature Name</th>
<th id="T_d0973_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">Definition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_d0973_row0_col0" class="data row0 col0">spectral_bandwidth_mean</td>
<td id="T_d0973_row0_col1" class="data row0 col1">How wide the sound is, from low to high; higher values often mean a brighter, more complex tone.</td>
</tr>
<tr class="even">
<td id="T_d0973_row1_col0" class="data row1 col0">tempo_std</td>
<td id="T_d0973_row1_col1" class="data row1 col1">How much the speed of the music varies; lower values mean steadier tempo, while higher values reflect more expressive or inconsistent timing.</td>
</tr>
<tr class="odd">
<td id="T_d0973_row2_col0" class="data row2 col0">chroma_stft_mean</td>
<td id="T_d0973_row2_col1" class="data row2 col1">Measures how often certain musical notes or chords show up; gives a sense of harmonic 'color'.</td>
</tr>
<tr class="even">
<td id="T_d0973_row3_col0" class="data row3 col0">onset_rate</td>
<td id="T_d0973_row3_col1" class="data row3 col1">Counts how frequently new notes or sounds begin, like how 'busy' or active the playing is.</td>
</tr>
<tr class="odd">
<td id="T_d0973_row4_col0" class="data row4 col0">spectral_centroid_mean</td>
<td id="T_d0973_row4_col1" class="data row4 col1">Where most of the sound energy is focused; higher values sound brighter, lower values sound darker.</td>
</tr>
<tr class="even">
<td id="T_d0973_row5_col0" class="data row5 col0">mfcc_3_mean</td>
<td id="T_d0973_row5_col1" class="data row5 col1">Captures details of tone and timbre, like a fingerprint of the sound texture.</td>
</tr>
<tr class="odd">
<td id="T_d0973_row6_col0" class="data row6 col0">mfcc_5_mean</td>
<td id="T_d0973_row6_col1" class="data row6 col1">Another layer of tonal detail; helps distinguish between human and machine playing styles.</td>
</tr>
<tr class="even">
<td id="T_d0973_row7_col0" class="data row7 col0">tonnetz_3_mean</td>
<td id="T_d0973_row7_col1" class="data row7 col1">Describes how chords and tones relate; tracks harmonic movement or tension.</td>
</tr>
<tr class="odd">
<td id="T_d0973_row8_col0" class="data row8 col0">tonnetz_6_mean</td>
<td id="T_d0973_row8_col1" class="data row8 col1">Also captures harmonic relationships, different from tonnetz_3 but similarly focused on musical structure.</td>
</tr>
<tr class="even">
<td id="T_d0973_row9_col0" class="data row9 col0">mfcc_delta2_13_mean</td>
<td id="T_d0973_row9_col1" class="data row9 col1">Tracks how the tonal texture changes over time; helps identify expressive or mechanical playing shifts.</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>To identify a concise and informative subset of acoustic features, we employed a rigorous, two-step selection approach that combined statistical hypothesis testing and model-based importance rankings. First, we used several univariate statistical tests, Welch’s t-tests, point-biserial correlations, and one-way ANOVA with post-hoc Tukey HSD, to measure the strength and consistency of differences between human-composed and AI-generated piano excerpts. Welch’s t-test was chosen because it robustly handles unequal variances and unequal sample sizes between groups. While some features deviated from normality under Shapiro–Wilk tests, Welch’s t-test is robust to moderate non-normality, especially with balanced samples. Point-biserial correlations are reported as descriptive effect sizes rather than formal significance tests. Sensitivity checks with nonparametric Mann–Whitney U tests produced qualitatively similar results. The point-biserial correlation, shown in Figure 2, quantified the strength of the linear relationship between each feature and the binary outcome (human or AI). The ANOVA (with Tukey post-hoc tests) confirmed statistically significant differences between group means while adjusting for multiple comparisons. Figure 1 displays the standardized mean differences of the final features that were selected, and whether or not the means are higher in the AI or human-composed classes.</p>
<div id="cell-fig-smd" class="cell" data-message="false" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-smd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-smd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="_04_data_files/figure-html/fig-smd-output-1.png" width="641" height="541" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-smd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Figure 1: Standardized mean differences (Human − AI) for the ten selected features.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure 1. Standardized mean differences (z-score) between human and AI excerpts for the ten selected features. Positive values (purple dots) indicate features whose mean is higher in human-performed clips, while negative values (yellow dots) indicate features higher in AI-generated clips. This visualization highlights the direction and magnitude of each feature’s discriminatory power. Note: see the appendix for more details on hypothesis test outcomes.</p>
<div id="cell-pb-correlations" class="cell" data-message="false" data-execution_count="4">
<div class="cell-output cell-output-display">
<div id="pb-correlations" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="_04_data_files/figure-html/pb-correlations-output-1.png" width="995" height="543" class="figure-img"></p>
<figcaption>Point-Biserial Correlations for Selected Features</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure 2. Point-biserial correlations between each selected acoustic feature and the binary class label. Bars extending to the right (purple) indicate features with higher average values in AI-generated excerpts, while bars extending to the left (yellow) indicate features with higher average values in human-performed excerpts.</p>
<p>Next, we compared feature rankings from three complementary methods, as shown in Figure 3. On the left, Cohen’s d quantifies each feature’s standardized effect size between human and AI clips, providing a model-agnostic measure of group differences. In the center, the absolute coefficients from an L1-penalized logistic regression pick out features with the strongest linear contributions to classification, enforcing sparsity to highlight only the most robust predictors. On the right, Random Forest Gini importance captures non-linear effects and interactions by measuring how much each feature reduces impurity across the ensemble’s decision trees. Including Cohen’s d alongside these model-based scores ensures we retain features that exhibit large, consistent differences in the raw data, even before any model fitting. Features were prioritized based on how consistently they ranked in the top 10 across multiple methods. However, it was important to keep in mind feature redundancy which we discuss in the next section.</p>
<p>Figure 3 Feature-importance bar charts</p>
<div id="cell-feature-importance-panels" class="cell" data-message="false" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="feature-importance-panels" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="_04_data_files/figure-html/feature-importance-panels-output-1.png" width="1980" height="638" class="figure-img"></p>
<figcaption>Figure 3: Top ten acoustic descriptors ranked by three complementary methods.</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure 3. Top ten acoustic descriptors selected from the full set of 58 features, ranked by three complementary methods. Left: Cohen’s d effect sizes from Welch’s t-tests; Center: absolute coefficients from an L1-penalized logistic regression; Right: Gini importances from a random forest. These paired comparisons show which features, spectral bandwidth, tempo variability, chroma balance, and others, emerged most consistently as discriminative for distinguishing human versus AI piano excerpts.</p>
<p>Three variables emerged as especially influential in distinguishing AI-generated from human performances. Spectral bandwidth, which reflects the range of frequencies present, tended to be narrower in human recordings, with a richer midrange produced by natural pedaling and touch. In contrast, AI clips often showed unnaturally extended highs and lows resulting from synthesis. Tempo variability captured natural fluctuations in timing, human performers frequently speed up or slow down for expressive effect, whereas AI outputs were more metronomic. Chroma balance, a measure of the evenness of pitch-class use, also differed between the two groups: human players often favored certain tonal centers or voicings, while AI tended toward a more uniform distribution across keys and harmonies. A Chi-square test found no significant association between feature distribution and generation platform (p &gt; 0.05), suggesting that differences are not platform-specific. Assumption Checks: Normality, Variance Homogeneity, and Multicollinearity</p>
<p>Before running parametric statistical tests such as Welch’s t-tests and ANOVA, we checked whether each feature met the assumptions of normality and equal variances between AI and human groups. Shapiro–Wilk tests indicated that all features deviated from normality in at least one group (p &lt; 0.05), so we did not assume normally distributed data. We also ran Levene’s tests for homogeneity of variances, which showed that several key features—such as spectral_bandwidth_mean, tempo_std, and mfcc_3_mean—had significantly different variances between classes. These results reinforced our choice of Welch’s t-test, which is robust to both non-normality and unequal variances, for all hypothesis testing. For the logistic regression model, we examined variance inflation factors (VIF) to assess multicollinearity among the ten final features. All VIF values were below the commonly used threshold of 5, indicating no problematic collinearity. This confirmed that each feature contributed unique information to the model and that coefficient estimates would remain stable and interpretable.</p>
<p>Feature Redundancy Check and Final Selection Criteria To reduce multicollinearity and improve interpretability, we computed pairwise Pearson correlations, Figure 4, among all top-ranked features and enforced a threshold of |r| &lt; 0.7, meaning no two selected features could be too closely correlated. The only exception was spectral_bandwidth_mean and spectral_centroid_mean, which were moderately correlated (r ≈ 0.73) but retained due to their distinct acoustic interpretations: bandwidth reflects spread of energy across frequencies, while centroid captures perceived brightness. Figure 4 Pairwise Pearson correlation heatmap for the ten final acoustic features</p>
<div id="cell-correlation-heatmap" class="cell" data-message="false" data-execution_count="6">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="_04_data_files/figure-html/correlation-heatmap-output-1.png" id="correlation-heatmap" width="849" height="733" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note. Shows all off-diagonal correlations below 0.7 (except the one justified exception), confirming non-redundancy of the selected feature set</p>
<p>This filtering step ensured that our final subset of ten acoustic features was both non-redundant and consistently predictive.</p>
<section id="variance-inflation-factor-vif-analysis" class="level3">
<h3 class="anchored" data-anchor-id="variance-inflation-factor-vif-analysis">Variance Inflation Factor (VIF) Analysis</h3>
<p>To further confirm that our selected features were not excessively collinear, we calculated Variance Inflation Factors (VIF) for each feature in the final set. VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors. A VIF of 1 indicates no correlation with other variables, while values above 5–10 are generally considered problematic. As shown in Table 2, all features had VIF values well below the threshold of 5, with most under 2, indicating minimal multicollinearity. This result supports our earlier Pearson correlation check (Figure 4) and provides additional assurance that each feature contributes unique information to the model without introducing instability into the coefficient estimates.</p>
<div class="cell" data-message="false" data-tbl-cap="Table 2: Variance Inflation Factor (VIF) for Final Features" data-execution_count="7">
<div id="table-vif" class="cell-output cell-output-display" data-execution_count="15">
<style type="text/css">
</style>

<table id="T_44899" class="caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<caption>Table 2: Variance Inflation Factor (VIF) for Final Features</caption>
<thead>
<tr class="header">
<th id="T_44899_level0_col0" class="col_heading level0 col0" data-quarto-table-cell-role="th">Feature</th>
<th id="T_44899_level0_col1" class="col_heading level0 col1" data-quarto-table-cell-role="th">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td id="T_44899_row0_col0" class="data row0 col0">spectral_bandwidth_mean</td>
<td id="T_44899_row0_col1" class="data row0 col1">3.682000</td>
</tr>
<tr class="even">
<td id="T_44899_row1_col0" class="data row1 col0">tempo_std</td>
<td id="T_44899_row1_col1" class="data row1 col1">1.291000</td>
</tr>
<tr class="odd">
<td id="T_44899_row2_col0" class="data row2 col0">chroma_stft_mean</td>
<td id="T_44899_row2_col1" class="data row2 col1">1.237000</td>
</tr>
<tr class="even">
<td id="T_44899_row3_col0" class="data row3 col0">onset_rate</td>
<td id="T_44899_row3_col1" class="data row3 col1">1.261000</td>
</tr>
<tr class="odd">
<td id="T_44899_row4_col0" class="data row4 col0">mfcc_3_mean</td>
<td id="T_44899_row4_col1" class="data row4 col1">1.525000</td>
</tr>
<tr class="even">
<td id="T_44899_row5_col0" class="data row5 col0">spectral_centroid_mean</td>
<td id="T_44899_row5_col1" class="data row5 col1">3.118000</td>
</tr>
<tr class="odd">
<td id="T_44899_row6_col0" class="data row6 col0">mfcc_5_mean</td>
<td id="T_44899_row6_col1" class="data row6 col1">1.128000</td>
</tr>
<tr class="even">
<td id="T_44899_row7_col0" class="data row7 col0">tonnetz_3_mean</td>
<td id="T_44899_row7_col1" class="data row7 col1">1.088000</td>
</tr>
<tr class="odd">
<td id="T_44899_row8_col0" class="data row8 col0">tonnetz_6_mean</td>
<td id="T_44899_row8_col1" class="data row8 col1">1.081000</td>
</tr>
<tr class="even">
<td id="T_44899_row9_col0" class="data row9 col0">mfcc_delta2_13_mean</td>
<td id="T_44899_row9_col1" class="data row9 col1">1.020000</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="exploratory-visualization-with-selected-features" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-visualization-with-selected-features">Exploratory Visualization with Selected Features</h3>
<p>To gain intuition about the separability of human-performed versus AI-generated piano excerpts in our selected feature space, we projected the ten features into two dimensions using t-distributed Stochastic Neighbor Embedding (t-SNE). As shown in Figure 4, each point represents a 30-second clip, colored yellow for humans and purple for AI.</p>
<div id="cell-tsne-selected" class="cell" data-message="false" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="tsne-selected" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="_04_data_files/figure-html/tsne-selected-output-1.png" width="732" height="543" class="figure-img"></p>
<figcaption>Figure 4: t-SNE projection of the ten selected features (purple = Human, gold = AI).</figcaption>
</figure>
</div>
</div>
</div>
<p>Figure 4. Two-dimensional t-SNE embedding of the ten selected acoustic features, with human-performed excerpts shown as yellow circles and AI-generated excerpts shown as purple circles. Pockets of yellow and purple illustrate regions where the acoustic signatures of human timing and timbre versus AI steadiness and spectral breadth tend to cluster.</p>
<p>While the two classes are not perfectly separated, reflecting overlap in certain acoustic traits, there are clear regions where human excerpts cluster, often marked by more expressive timing variations and narrower spectral spreads. AI-generated clips, in contrast, frequently group together in areas characterized by steadier tempi and broader spectral energy. Notably, outlier analysis revealed that most extreme cases in the AI class came from the Suno platform, which tended to produce pieces with unusually high spectral bandwidth and onset rates. These characteristics explain much of the tight clustering for Suno-generated excerpts, while other AI platforms showed more dispersion. Occasional overlaps between classes appear to stem from individual pieces whose acoustic profiles happen to align, rather than random noise, reinforcing the need for a supervised model to capture subtle but consistent distinctions across platforms.</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>