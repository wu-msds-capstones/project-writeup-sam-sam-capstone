---
title: "Capstone Project"
subtitle: Subtitle
authors:
  - Sam
title-block-banner: true
abstract: >
  Distinguishing AI-generated from human-performed music is challenging due to differences in instrumentation, style, and production techniques. To minimize these confounding factors, we focused exclusively on 30-second solo piano excerpts, assembling a balanced dataset of 722 clips (361 human, 361 AI). After extracting acoustic descriptors related to spectral content, rhythm, harmony, and timbre, we identified ten highly predictive features through statistical testing and model-based selection. We trained an enhanced stacking ensemble classifier combining logistic regression, random forest, gradient boosting, and naïve Bayes methods. On an unseen hold-out set, the stacking model achieved an accuracy of 82.1% (ROC-AUC 0.924). The strongest predictors included spectral bandwidth, tempo variability, and chroma balance, highlighting clear acoustic signatures associated with human musical performance. These findings offer a transparent and scalable foundation for automated detection of AI-generated music.
format:
  html:
    theme:
      light: lightly
      dark: darkly
    toc: true
    number-sections: true
---

# Introduction

The rapid advancement of generative AI models has significantly lowered the barrier for creating music, raising concerns about transparency, authenticity, and impacts on human composers. Commercial streaming platforms currently lack reliable methods to differentiate between machine-generated and human-performed tracks, which may negatively affect listener trust and disadvantage musicians.

For instance, the widespread popularity of AI-produced albums, such as the recent viral success of *The Velvet Sundown* in June 2025, demonstrates the increasing prevalence and sophistication of AI-generated music. Although many AI-produced works can be easily identified through visual or contextual clues, automated detection based solely on audio remains challenging, particularly when controlling for confounding factors such as instrumentation, style, and mixing.

This study addresses a specific aspect of this broader challenge by investigating whether standard acoustic features can reliably distinguish between human-performed and AI-generated solo piano music. Solo piano excerpts were chosen specifically to control for differences in instrumentation, ensemble size, and production effects, providing a clearer basis for analysis.

A balanced dataset of 722 piano clips (361 human, 361 AI) was compiled from public-domain performances (FMA, Musopen) and several generative models (AIVA, MuseNet, Udio, and two additional emergent systems). All clips were standardized (44.1 kHz mono WAV at –23 LUFS), and we extracted 55 acoustic descriptors covering spectral, temporal, harmonic, and cepstral domains. Statistical testing and model-based rankings then yielded a subset of ten non-redundant, informative features. Finally, those features were used to train and evaluate a calibrated stacking ensemble classifier via 5-fold cross-validation and a held-out test set (80/20 split).

# Background

Differentiating AI-generated from human-composed music is a growing focus of research, spanning both listener perception and computational detection methods. Recent perceptual studies (Collins et al., 2023; Sarmento et al., 2024) have found that listeners generally struggle to distinguish AI-generated music from human-performed pieces, yet tend to express a clear preference for human performances.

Computational detection techniques have achieved strong results. For example, deep neural networks trained directly on audio spectrograms (Afchar et al., 2025; Vernet et al., 2025) and transformer-based methods (Independent Project, 2022) have reported accuracies above 95%. However, these approaches typically lack transparency due to their complexity and reliance on abstract learned representations.

Alternatively, simpler acoustic features derived directly from audio signals offer greater interpretability. Features such as spectral centroids and Mel-Frequency Cepstral Coefficients (MFCCs) effectively describe differences in timbre between AI-generated and human-composed music (Dervakos et al., 2021). Temporal descriptors, including tempo variability and onset rate, reflect human expressive timing patterns and subtle rhythmic fluctuations (Flexer et al., 2006; Sarmento et al., 2024). Harmonic descriptors (e.g., Tonnetz features, chroma balance) further characterize structural differences in tonal and harmonic organization.

This current project aims to contribute to the current research by focusing on the differences in acoustic features between AI-generated and human-composed 30-second solo piano pieces, and to create a classification model that is able to reliably predict whether or not a 30-second solo piano piece is AI-generated or composed by a human. We hypothesized that human performances would exhibit greater variability in timing and harmonic structure, while AI-generated excerpts would display more consistent tempo and broader spectral energy distributions.

# Data and Methodology

## Data Acquisition
Data for this project was gathered from multiple sources. The human-composed solo piano music was gathered from two different open source music archives including: Free Music Archive and Musopen. The AI-generated music was manually sourced by utilizing free web-based AI music generators: AIVA, Suno, Udio, Riffusion, and DeepAI.

To ensure replicability, AI clips were generated using minimal, piano-only prompts such as "solo piano, only piano" and slight variations (e.g., "solo piano performance", "classical solo piano"). These were chosen after pilot testing showed they produced the highest proportion of pure piano output without extra instrumentation.

Pieces were included only if they met quality standards:
- **Instrument purity** — no audible non-piano sounds.
- **Clarity** — no distortion, clipping, or excessive background noise.
- **Adequate dynamic range** — clear differences between soft and loud passages.
- **Sufficient volume** — must normalize without introducing artifacts.

This was important because spectral and harmonic features can be influenced by recording quality; setting strict criteria reduced the likelihood that observed differences were due to microphone, mixing, or mastering factors rather than performance or composition.

## Data Processing
After downloading human and AI-generated solo piano pieces from their respective sources, several preprocessing steps were taken to prepare a consistent and balanced dataset. A Python script was utilized to standardize each audio file to mono 16-bit WAV format at 44.1 kHz sampling rate and normalized loudness to -23 LUFS, eliminating differences related to audio quality and loudness. To ensure sufficient sample size and balanced representation, especially since AI-generated pieces tended to be shorter, we randomly extracted up to four 30-second excerpts from each AI-generated track, and up to two excerpts from each human-performed track. These excerpts were selected by dividing each piece into non-overlapping excerpts and randomly sampling among them.

From each excerpt, we extracted a variety of acoustic features covering four broad categories:
- **Spectral features** capture the overall sound color or timbre of the piano, reflecting characteristics such as brightness and tonal complexity.  
- **Temporal features** describe rhythmic and timing aspects, such as how consistently or expressively the performer maintains tempo.  
- **Harmonic features** quantify the musical relationships between notes and chords, providing insights into the structural and tonal qualities of the music.  
- **Cepstral features** represent subtle variations in sound texture, helping to distinguish nuanced differences in tone and articulation between human and AI performances.  

Collectively, these acoustic features allowed us to systematically characterize the musical differences between human-performed and AI-generated piano excerpts. The resulting data set includes 58 acoustic features from a balanced number of 722 30-second excerpts, 361 of which were AI-generated excerpts and 361 of which were human-composed excerpts.

## Data Engineering
We organized our dataset into three clearly defined tables, each stored separately to facilitate straightforward analysis without unnecessary merging.

- A **metadata table** was created to ensure traceability and reproducibility. Every audio excerpt was assigned a unique identifier, linking it consistently across tables and enabling accurate referencing of the original sources and associated features.
- The **full dataset** was generated directly by a Python script that extracted 58 acoustic features from each audio clip. This dataset preserved all raw features without alteration, providing a comprehensive view of each excerpt’s acoustic profile.
- Lastly, a **streamlined final feature dataset** was created, containing only the ten features identified as most predictive during feature selection. This reduced dataset served as the primary input for modeling, ensuring clarity, interpretability, and computational efficiency.

## Feature Selection & Statistical Analysis
Used in the final models was a group of ten features. Table 1 provides an overview of these features and their definitions.

### Table 1. Features used in the final model and their definitions
| Feature Name                | Definition |
|-----------------------------|------------|
| spectral_bandwidth_mean     | How wide the sound is, from low to high; higher values often mean a brighter, more complex tone. |
| tempo_std                    | How much the speed of the music varies; lower values mean steadier tempo, while higher values reflect more expressive or inconsistent timing. |
| chroma_stft_mean             | Measures how often certain musical notes or chords show up; gives a sense of harmonic "color." |
| onset_rate                   | Counts how frequently new notes or sounds begin, like how “busy” or active the playing is. |
| spectral_centroid_mean       | Where most of the sound energy is focused; higher values sound brighter, lower values sound darker. |
| mfcc_3_mean                  | Captures details of tone and timbre, like a fingerprint of the sound texture. |
| mfcc_5_mean                  | Another layer of tonal detail; helps distinguish between human and machine playing styles. |
| tonnetz_3_mean               | Describes how chords and tones relate; tracks harmonic movement or tension. |
| tonnetz_6_mean               | Also captures harmonic relationships, different from tonnetz_3 but similarly focused on musical structure. |
| mfcc_delta2_13_mean          | Tracks how the tonal texture changes over time; helps identify expressive or mechanical playing shifts. |

To identify a concise and informative subset of acoustic features, we employed a approach that combined statistical hypothesis testing and model-based importance rankings.

First, we used several univariate statistical tests — Welch’s *t*-tests, point-biserial correlations, and one-way ANOVA with post-hoc Tukey HSD — to measure the strength and consistency of differences between human-composed and AI-generated piano excerpts. Welch’s *t*-test was chosen because it robustly handles unequal variances and unequal sample sizes between groups.  

While some features deviated from normality under Shapiro–Wilk tests, Welch’s *t*-test is robust to moderate non-normality, especially with balanced samples. Point-biserial correlations are reported as descriptive effect sizes rather than formal significance tests. Sensitivity checks with nonparametric Mann–Whitney U tests produced qualitatively similar results.  

The point-biserial correlation, shown in **Figure @fig-pb**, quantified the strength of the linear relationship between each feature and the binary outcome (human or AI). The ANOVA (with Tukey post-hoc tests) confirmed statistically significant differences between group means while adjusting for multiple comparisons. **Figure @fig-meandiff** displays the standardized mean differences of the final features that were selected, and whether or not the means are higher in the AI or human-composed classes.

![Figure 1: Standardized mean differences (z-score) between human and AI excerpts for the ten selected features.](Visualizations/meandifbyfeature.png){#fig-meandiff width=600}

![Figure 2: Point-biserial correlations between each selected acoustic feature and the binary class label.](Visualizations/pointbiserial.png){#fig-pb width=600}

Next, we compared feature rankings from three complementary methods, as shown in **Figure @fig-featimp**.

![Figure 3: Top ten acoustic descriptors selected from the full set of 58 features, ranked by three complementary methods.](Visualizations/featureimportance.png){#fig-featimp width=600}

Three variables emerged as especially influential in distinguishing AI-generated from human performances:  
- **Spectral bandwidth** …  
- **Tempo variability** …  
- **Chroma balance** …  

A Chi-square test found no significant association between feature distribution and generation platform (*p* > 0.05), suggesting that differences are not platform-specific.

# Results

## Dataset Summary
The final dataset contained 722 thirty-second solo piano excerpts, evenly split between human-composed (n = 361) and AI-generated (n = 361) excerpts. Human recordings were drawn from public-domain libraries (FMA and Musopen) spanning classical, jazz, and contemporary styles. AI-generated examples came from four systems (AIVA, Udio, and two emergent models) to capture a range of algorithmic approaches. All source tracks exceeded 30 seconds, and metadata (platform, license, creator or generator name) was recorded for traceability.

## Model Choice and Pipeline
Initial experiments with individual classifiers (logistic regression, random forest, gradient boosting) revealed trade-offs between interpretability and performance. We ultimately settled on a stacking model combining calibrated logistic regression, random forest (100 trees, max_depth = 5), gradient boosting, and naïve Bayes base learners. Pairwise interaction terms were added via second-degree polynomial features, and a logistic regression meta-learner was used to integrate their outputs. Stratified five-fold cross-validation guided hyperparameter selection and helped limit overfitting, while a final hold-out evaluation provided an unbiased assessment.

## Model Evaluation

### Table 3. Hold-Out Test Results
| Metric     | Value |
|------------|-------|
| Accuracy   | 0.821 |
| Precision  | 0.803 |
| Recall     | 0.847 |
| F1 Score   | 0.824 |
| ROC-AUC    | 0.924 |

**Confusion matrix** is shown in **Figure @fig-confmat**.

![Figure 6: Confusion matrix showing counts of true human and AI excerpts versus the model’s human/AI predictions.](Visualizations/confusionmatrix.png){#fig-confmat width=400}

**ROC curve** is shown in **Figure @fig-roc**.

![Figure 7: ROC curve (AUC = 0.924) plotting True Positive Rate vs. False Positive Rate; dashed line = random classifier.](Visualizations/ROCcurve.png){#fig-roc width=600}

### Training vs. Test Performance
On the training set, the logistic regression model achieved near-perfect performance … (table omitted here for brevity — paste your existing section if desired)

## Error Analysis

![Figure 8: Error analysis highlighting types of misclassifications.](Visualizations/erroranalysis.png){#fig-error width=600}

(Your narrative text here — same as before.)

# Discussion

(Your Discussion section as before.)

# Appendix

(Your Appendix tables as before.)

# References

(Your References list as before — exactly as you pasted earlier.)
