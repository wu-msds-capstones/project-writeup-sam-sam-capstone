# Methods
Data Acquisition. 

Data for this project was gathered from multiple sources. The human-composed solo piano music was gathered from two different open source music archives including: Free Music Archive, and Musopen. The AI-generated music was manually sourced by utilizing free web based AI music generators: AIVA, Suno, Udio, Riffusion, and deepAI. To be included in the dataset every piece had to be of a certain quality and only contain sounds coming from a piano. Pieces that included background noise, audio distortions, or if the music was faint, meant that the piece did not meet the quality standards to be included in the data set.

While the human-composed solo piano pieces already existed, the AI-generated music was generated solely for the purpose of this project. The AI-generated pieces were all generated via a prompt such as “solo piano, only piano,” and other key words with the intention of reflecting the human-composed music used in the dataset. Prompts such as this were used because it led to more pieces containing only piano than more descriptive prompts. The reason the pieces were generated for this project was to minimize the fact that we would not know how the pieces were generated.
Data Processing

After downloading human and AI-generated solo piano pieces from their respective sources, several preprocessing steps were taken to prepare a consistent and balanced dataset. A Python script was utilized to standardize each audio file to mono 16-bit WAV format at 44.1 kHz sampling rate and normalized loudness to -23 LUFS, eliminating differences related to audio quality and loudness. To ensure sufficient sample size and balanced representation, especially since AI-generated pieces tended to be shorter, we randomly extracted up to four 30-second excerpts from each AI-generated track, and up to two excerpts from each human-performed track. These excerpts were selected by dividing each piece into non-overlapping excerpts and randomly sampling among them.

From each excerpt, we extracted a variety of acoustic features covering four broad categories. Spectral features capture the overall sound color or timbre of the piano, reflecting characteristics such as brightness and tonal complexity. Temporal features describe rhythmic and timing aspects, such as how consistently or expressively the performer maintains tempo. Harmonic features quantify the musical relationships between notes and chords, providing insights into the structural and tonal qualities of the music. Finally, cepstral features represent subtle variations in sound texture, helping to distinguish nuanced differences in tone and articulation between human and AI performances. Collectively, these acoustic features allowed us to systematically characterize the musical differences between human-performed and AI-generated piano excerpts.
The resulting data set includes 58 acoustic features from a balanced number of 722 30 second excerpts, 361 of which were AI-generated excerpts and 361 of which were human-composed excerpts.

Feature Selection & Statistical Analysis

Used in the final models was a group of ten features. Table 1 provides an overview of these features and their definitions.


To identify a concise and informative subset of acoustic features, we employed a rigorous, two-step selection approach that combined statistical hypothesis testing and model-based importance rankings.

First, we used several univariate statistical tests, Welch’s t-tests, point-biserial correlations, and one-way ANOVA with post-hoc Tukey HSD, to measure the strength and consistency of differences between human-composed and AI-generated piano excerpts. Welch’s t-test was chosen because it robustly handles unequal variances and unequal sample sizes between groups. The point-biserial correlation, shown in Figure 2, quantified the strength of the linear relationship between each feature and the binary outcome (human or AI). The ANOVA (with Tukey post-hoc tests) confirmed statistically significant differences between group means while adjusting for multiple comparisons. Figure 1 displays the standardized mean differences of the final features that were selected, and whether or not the means are higher in the AI or human-composed classes.

Next, we compared feature rankings from three complementary methods, as shown in Figure 3. On the left, Cohen’s d quantifies each feature’s standardized effect size between human and AI clips, providing a model-agnostic measure of group differences. In the center, the absolute coefficients from an L1-penalized logistic regression pick out features with the strongest linear contributions to classification, enforcing sparsity to highlight only the most robust predictors. On the right, Random Forest Gini importance captures non-linear effects and interactions by measuring how much each feature reduces impurity across the ensemble’s decision trees. Including Cohen’s d alongside these model-based scores ensures we retain features that exhibit large, consistent differences in the raw data, even before any model fitting.

Features were prioritized based on how consistently they ranked in the top 10 across multiple methods. However, it was important to keep in mind feature redundancy which we discuss in the next section.

Feature Redundancy Check and Final Selection Criteria

To reduce multicollinearity and improve interpretability, we computed pairwise Pearson correlations, Figure 4, among all top-ranked features and enforced a threshold of |r| < 0.7, meaning no two selected features could be too closely correlated. The only exception was spectral_bandwidth_mean and spectral_centroid_mean, which were moderately correlated (r ≈ 0.73) but retained due to their distinct acoustic interpretations: bandwidth reflects spread of energy across frequencies, while centroid captures perceived brightness.

This filtering step ensured that our final subset of ten acoustic features was both non-redundant and consistently predictive. Exploratory Visualization with Selected Features

To gain intuition about the separability of human-performed versus AI-generated piano excerpts in our selected feature space, we projected the ten features into two dimensions using t-distributed Stochastic Neighbor Embedding (t-SNE). As shown in Figure 4, each point represents a 30-second clip, colored yellow for humans and purple for AI.

Although the two classes are not perfectly separated—which reflects the inherent overlap in some acoustic characteristics—you can see distinct “pockets” where human excerpts cluster (often those with more expressive timing fluctuations and narrower spectral spreads) and where AI clips cluster (tending toward steadier tempi and broader spectral energy). This partial clustering confirms that our ten features capture meaningful differences, and it motivates the use of a supervised model to formalize the discrimination.

We observed that some AI-generated excerpts fell into the same regions of the t-SNE plot as human performances, but we did not identify any clear, consistent reason why certain clips overlapped while others did not. No single feature or platform fully explains these overlaps, suggesting that both human and AI outputs can share similar acoustic profiles under certain conditions. At this stage, we conclude that the occasional intermingling of clusters reflects natural variation in playing style and generative model behavior rather than a systematic bias in our feature set.





