<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Using Machine Learning to Classify AI‑ and Human‑Composed Solo Piano Pieces</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-2fef5ea3f8957b3e4ecc936fc74692ca.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6bdd2aebeb936dcddaa5f935a5de481c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-ccd81253fefb92e9cd3f9eb5b5d87b54.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-6bdd2aebeb936dcddaa5f935a5de481c.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Using Machine Learning to Classify AI‑ and Human‑Composed Solo Piano Pieces</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p>Sam Beilenson </p>
               <p>Sam Twenhafel </p>
            </div>
    </div>
      
    
      
    </div>
    
  <div>
    <div class="abstract">
      <div class="block-title">Abstract</div>
      <p>Distinguishing AI-generated from human-performed music is challenging due to differences in instrumentation, style, and production techniques. To minimize these confounding factors, we focused exclusively on 30-second solo piano excerpts, assembling a balanced dataset of 722 clips (361 human, 361 AI). After extracting acoustic descriptors related to spectral content, rhythm, harmony, and timbre, we identified ten highly predictive features through statistical testing and model-based selection. We trained an enhanced stacking ensemble classifier combining logistic regression, random forest, gradient boosting, and naïve Bayes methods. On an unseen hold-out set, the stacking model achieved an accuracy of 82.1% (ROC-AUC 0.924). The strongest predictors included spectral bandwidth, tempo variability, and chroma balance, highlighting clear acoustic signatures associated with human musical performance. These findings offer a transparent and scalable foundation for automated detection of AI-generated music.</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">2</span> Background</a></li>
  <li><a href="#data-and-methodology" id="toc-data-and-methodology" class="nav-link" data-scroll-target="#data-and-methodology"><span class="header-section-number">3</span> Data and Methodology</a>
  <ul class="collapse">
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition"><span class="header-section-number">3.1</span> Data Acquisition</a></li>
  <li><a href="#data-processing" id="toc-data-processing" class="nav-link" data-scroll-target="#data-processing"><span class="header-section-number">3.2</span> Data Processing</a></li>
  <li><a href="#data-engineering" id="toc-data-engineering" class="nav-link" data-scroll-target="#data-engineering"><span class="header-section-number">3.3</span> Data Engineering</a></li>
  <li><a href="#feature-selection-statistical-analysis" id="toc-feature-selection-statistical-analysis" class="nav-link" data-scroll-target="#feature-selection-statistical-analysis"><span class="header-section-number">3.4</span> Feature Selection &amp; Statistical Analysis</a></li>
  <li><a href="#assumption-checks-normality-variance-homogeneity-and-multicollinearity" id="toc-assumption-checks-normality-variance-homogeneity-and-multicollinearity" class="nav-link" data-scroll-target="#assumption-checks-normality-variance-homogeneity-and-multicollinearity"><span class="header-section-number">3.5</span> Assumption Checks: Normality, Variance Homogeneity, and Multicollinearity</a></li>
  <li><a href="#feature-redundancy-check-and-final-selection-criteria" id="toc-feature-redundancy-check-and-final-selection-criteria" class="nav-link" data-scroll-target="#feature-redundancy-check-and-final-selection-criteria"><span class="header-section-number">3.6</span> Feature Redundancy Check and Final Selection Criteria</a></li>
  <li><a href="#variance-inflation-factor-vif-analysis" id="toc-variance-inflation-factor-vif-analysis" class="nav-link" data-scroll-target="#variance-inflation-factor-vif-analysis"><span class="header-section-number">3.7</span> Variance Inflation Factor (VIF) Analysis</a></li>
  <li><a href="#exploratory-visualization-with-selected-features" id="toc-exploratory-visualization-with-selected-features" class="nav-link" data-scroll-target="#exploratory-visualization-with-selected-features"><span class="header-section-number">3.8</span> Exploratory Visualization with Selected Features</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">4</span> Results</a>
  <ul class="collapse">
  <li><a href="#dataset-summary" id="toc-dataset-summary" class="nav-link" data-scroll-target="#dataset-summary"><span class="header-section-number">4.1</span> Dataset Summary</a></li>
  <li><a href="#model-choice-and-pipeline" id="toc-model-choice-and-pipeline" class="nav-link" data-scroll-target="#model-choice-and-pipeline"><span class="header-section-number">4.2</span> Model Choice and Pipeline</a></li>
  <li><a href="#model-evaluation" id="toc-model-evaluation" class="nav-link" data-scroll-target="#model-evaluation"><span class="header-section-number">4.3</span> Model Evaluation</a></li>
  <li><a href="#model-performance-and-generalization" id="toc-model-performance-and-generalization" class="nav-link" data-scroll-target="#model-performance-and-generalization"><span class="header-section-number">4.4</span> Model Performance and Generalization</a></li>
  <li><a href="#error-analysis" id="toc-error-analysis" class="nav-link" data-scroll-target="#error-analysis"><span class="header-section-number">4.5</span> Error Analysis</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">5</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#acoustic-cues-of-humanness" id="toc-acoustic-cues-of-humanness" class="nav-link" data-scroll-target="#acoustic-cues-of-humanness"><span class="header-section-number">5.1</span> Acoustic Cues of “Humanness”</a></li>
  <li><a href="#error-modes-and-model-boundaries" id="toc-error-modes-and-model-boundaries" class="nav-link" data-scroll-target="#error-modes-and-model-boundaries"><span class="header-section-number">5.2</span> Error Modes and Model Boundaries</a></li>
  <li><a href="#data-ethics" id="toc-data-ethics" class="nav-link" data-scroll-target="#data-ethics"><span class="header-section-number">5.3</span> Data Ethics</a></li>
  <li><a href="#directions-for-future-work" id="toc-directions-for-future-work" class="nav-link" data-scroll-target="#directions-for-future-work"><span class="header-section-number">5.4</span> Directions for Future Work</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix"><span class="header-section-number">6</span> Appendix:</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">






<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The rapid advancement of generative AI models has significantly lowered the barrier for creating music, raising concerns about transparency, authenticity, and impacts on human composers. Commercial streaming platforms currently lack reliable methods to differentiate between machine-generated and human-performed tracks, which may negatively affect listener trust and disadvantage musicians. For instance, the widespread popularity of AI-produced albums, such as the recent viral success of The Velvet Sundown in June 2025, demonstrates the increasing prevalence and sophistication of AI-generated music. Although many AI-produced works can be easily identified through visual or contextual clues, automated detection based solely on audio remains challenging, particularly when controlling for confounding factors such as instrumentation, style, and mixing. This study addresses a specific aspect of this broader challenge by investigating whether standard acoustic features can reliably distinguish between human-performed and AI-generated solo piano music. Solo piano excerpts were chosen specifically to control for differences in instrumentation, ensemble size, and production effects, providing a clearer basis for analysis. A balanced dataset of 722 piano clips (361 human, 361 AI) was compiled from public-domain performances (FMA, Musopen) and several generative models (AIVA, MuseNet, Udio, and two additional emergent systems). All clips were standardized (44.1 kHz mono WAV at –23 LUFS), and we extracted 55 acoustic descriptors covering spectral, temporal, harmonic, and cepstral domains. Statistical testing and model-based rankings then yielded a subset of ten non-redundant, informative features. Finally those features were used to train and evaluate a calibrated stacking ensemble classifier via 5-fold cross-validation and a held-out test set (80/20 split).</p>
</section>
<section id="background" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Background</h1>
<p>Differentiating AI-generated from human-composed music is a growing focus of research, spanning both listener perception and computational detection methods. Recent perceptual studies (Collins et al., 2023; Sarmento et al., 2024) have found that listeners generally struggle to distinguish AI-generated music from human-performed pieces, yet tend to express a clear preference for human performances. Computational detection techniques have achieved strong results. For example, deep neural networks trained directly on audio spectrograms (Afchar et al., 2025; Vernet et al., 2025) and transformer-based methods (Independent Project, 2022) have reported accuracies above 95%. However, these approaches typically lack transparency due to their complexity and reliance on abstract learned representations. Alternatively, simpler acoustic features derived directly from audio signals offer greater interpretability. Features such as spectral centroids and Mel-Frequency Cepstral Coefficients (MFCCs) effectively describe differences in timbre between AI-generated and human-composed music (Dervakos et al., 2021). Temporal descriptors, including tempo variability and onset rate, reflect human expressive timing patterns and subtle rhythmic fluctuations (Flexer et al., 2006; Sarmento et al., 2024). Harmonic descriptors (e.g., Tonnetz features, chroma balance) further characterize structural differences in tonal and harmonic organization. This current project aims to contribute to the current research by focusing on the differences in acoustic features between AI-generated and human-composed 30 second solo piano pieces, and to create a classification model that is able to reliably predict whether or not a 30 second solo piano piece is AI-generated or composed by a human. We hypothesized that human performances would exhibit greater variability in timing and harmonic structure, while AI-generated excerpts would display more consistent tempo and broader spectral energy distributions.</p>
</section>
<section id="data-and-methodology" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Data and Methodology</h1>
<section id="data-acquisition" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="data-acquisition"><span class="header-section-number">3.1</span> Data Acquisition</h2>
<p>Data for this project was gathered from multiple sources. The human-composed solo piano music was gathered from two different open source music archives including: Free Music Archive and Musopen. The AI-generated music was manually sourced by utilizing free web-based AI music generators: AIVA, Suno, Udio, Riffusion, and DeepAI.</p>
<p>To ensure replicability, AI clips were generated using minimal, piano-only prompts such as “solo piano, only piano” and slight variations (e.g., “solo piano performance”, “classical solo piano”). These were chosen after pilot testing showed they produced the highest proportion of pure piano output without extra instrumentation.</p>
<p>Pieces were included only if they met quality standards:</p>
<ul>
<li><strong>Instrument purity</strong> — no audible non-piano sounds.<br>
</li>
<li><strong>Clarity</strong> — no distortion, clipping, or excessive background noise.<br>
</li>
<li><strong>Adequate dynamic range</strong> — clear differences between soft and loud passages.<br>
</li>
<li><strong>Sufficient volume</strong> — must normalize without introducing artifacts.</li>
</ul>
<p>This was important because spectral and harmonic features can be influenced by recording quality; setting strict criteria reduced the likelihood that observed differences were due to microphone, mixing, or mastering factors rather than performance or composition.</p>
</section>
<section id="data-processing" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="data-processing"><span class="header-section-number">3.2</span> Data Processing</h2>
<p>After downloading human and AI-generated solo piano pieces from their respective sources, several preprocessing steps were taken to prepare a consistent and balanced dataset. A Python script was utilized to standardize each audio file to mono 16-bit WAV format at 44.1 kHz sampling rate and normalized loudness to -23 LUFS, eliminating differences related to audio quality and loudness. To ensure sufficient sample size and balanced representation, especially since AI-generated pieces tended to be shorter, we randomly extracted up to four 30-second excerpts from each AI-generated track, and up to two excerpts from each human-performed track. These excerpts were selected by dividing each piece into non-overlapping excerpts and randomly sampling among them.</p>
<p>From each excerpt, we extracted a variety of acoustic features covering four broad categories:</p>
<ul>
<li><strong>Spectral features</strong> capture the overall sound color or timbre of the piano, reflecting characteristics such as brightness and tonal complexity.<br>
</li>
<li><strong>Temporal features</strong> describe rhythmic and timing aspects, such as how consistently or expressively the performer maintains tempo.<br>
</li>
<li><strong>Harmonic features</strong> quantify the musical relationships between notes and chords, providing insights into the structural and tonal qualities of the music.<br>
</li>
<li><strong>Cepstral features</strong> represent subtle variations in sound texture, helping to distinguish nuanced differences in tone and articulation between human and AI performances.</li>
</ul>
<p>Collectively, these acoustic features allowed us to systematically characterize the musical differences between human-performed and AI-generated piano excerpts. The resulting data set includes 58 acoustic features from a balanced number of 722 30-second excerpts, 361 of which were AI-generated excerpts and 361 of which were human-composed excerpts.</p>
</section>
<section id="data-engineering" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="data-engineering"><span class="header-section-number">3.3</span> Data Engineering</h2>
<p>We organized our dataset into three clearly defined tables, each stored separately to facilitate straightforward analysis without unnecessary merging.</p>
<p>A metadata table was created to ensure traceability and reproducibility. Every audio excerpt was assigned a unique identifier, linking it consistently across tables and enabling accurate referencing of the original sources and associated features.</p>
<p>The full dataset was generated directly by a Python script that extracted 58 acoustic features from each audio clip. This dataset preserved all raw features without alteration, providing a comprehensive view of each excerpt’s acoustic profile.</p>
<p>Lastly, a streamlined final feature dataset was created, containing only the ten features identified as most predictive during feature selection. This reduced dataset served as the primary input for modeling, ensuring clarity, interpretability, and computational efficiency.</p>
</section>
<section id="feature-selection-statistical-analysis" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="feature-selection-statistical-analysis"><span class="header-section-number">3.4</span> Feature Selection &amp; Statistical Analysis</h2>
<p>Used in the final models was a group of ten features. Table 1 provides an overview of these features and their definitions.</p>
<p>Table 1. Features used in the final model and their definitions | Feature Name | Definition | |—————————–|————| | spectral_bandwidth_mean | How wide the sound is, from low to high; higher values often mean a brighter, more complex tone. | | tempo_std | How much the speed of the music varies; lower values mean steadier tempo, while higher values reflect more expressive or inconsistent timing. | | chroma_stft_mean | Measures how often certain musical notes or chords show up; gives a sense of harmonic “color.” | | onset_rate | Counts how frequently new notes or sounds begin, like how “busy” or active the playing is. | | spectral_centroid_mean | Where most of the sound energy is focused; higher values sound brighter, lower values sound darker. | | mfcc_3_mean | Captures details of tone and timbre, like a fingerprint of the sound texture. | | mfcc_5_mean | Another layer of tonal detail; helps distinguish between human and machine playing styles. | | tonnetz_3_mean | Describes how chords and tones relate; tracks harmonic movement or tension. | | tonnetz_6_mean | Also captures harmonic relationships, different from tonnetz_3 but similarly focused on musical structure. | | mfcc_delta2_13_mean | Tracks how the tonal texture changes over time; helps identify expressive or mechanical playing shifts. |</p>
<p>To identify a concise and informative subset of acoustic features, we employed an approach that combined statistical hypothesis testing and model-based importance rankings.</p>
<p>First, we used several univariate statistical tests, Welch’s <em>t</em>-tests, point-biserial correlations, and one-way ANOVA with post-hoc Tukey HSD, to measure the strength and consistency of differences between human-composed and AI-generated piano excerpts. Welch’s <em>t</em>-test was chosen because it robustly handles unequal variances and unequal sample sizes between groups.</p>
<p>While some features deviated from normality under Shapiro–Wilk tests, Welch’s <em>t</em>-test is robust to moderate non-normality, especially with balanced samples. Point-biserial correlations are reported as descriptive effect sizes rather than formal significance tests. Sensitivity checks with nonparametric Mann–Whitney U tests produced qualitatively similar results.</p>
<p>The point-biserial correlation, shown in <a href="#fig-pb" class="quarto-xref">Figure&nbsp;2</a>, quantified the strength of the linear relationship between each feature and the binary outcome (human or AI). The ANOVA (with Tukey post-hoc tests) confirmed statistically significant differences between group means while adjusting for multiple comparisons. <a href="#fig-meandiff" class="quarto-xref">Figure&nbsp;1</a> displays the standardized mean differences of the final features that were selected, and whether or not the means are higher in the AI or human-composed classes.</p>
<div id="fig-meandiff" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-meandiff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/meandifbyfeature.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-meandiff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Standardized mean differences (z-scores) between human and AI excerpts for the ten selected features, with bar colors indicating which class had the higher mean.
</figcaption>
</figure>
</div>
<div id="fig-pb" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/pointbiserial.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Point-biserial correlations between each selected acoustic feature and the binary class label.
</figcaption>
</figure>
</div>
<p>Next, we compared feature rankings from three complementary methods, as shown in <a href="#fig-featimp" class="quarto-xref">Figure&nbsp;3</a>. On the left, Cohen’s d quantifies each feature’s standardized effect size between human and AI clips, providing a model-agnostic measure of group differences. In the center, the absolute coefficients from an L1-penalized logistic regression pick out features with the strongest linear contributions to classification, enforcing sparsity to highlight only the most robust predictors. On the right, Random Forest Gini importance captures non-linear effects and interactions by measuring how much each feature reduces impurity across the ensemble’s decision trees. Including Cohen’s d alongside these model-based scores ensures we retain features that exhibit large, consistent differences in the raw data, even before any model fitting. Features were prioritized based on how consistently they ranked in the top 10 across multiple methods. However, it was important to keep in mind feature redundancy which we discuss in the next section.</p>
<div id="fig-featimp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-featimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/featureimportance.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-featimp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Top ten acoustic descriptors selected from the full set of 58 features, ranked by Cohen’s d, logistic regression coefficients, and Random Forest Gini importance.
</figcaption>
</figure>
</div>
<p>Three variables emerged as especially influential in distinguishing AI-generated from human performances. Spectral bandwidth, which reflects the range of frequencies present, tended to be narrower in human recordings, with a richer midrange produced by natural pedaling and touch. In contrast, AI clips often showed unnaturally extended highs and lows resulting from synthesis. Tempo variability captured natural fluctuations in timing, human performers frequently speed up or slow down for expressive effect, whereas AI outputs were more metronomic. Chroma balance, a measure of the evenness of pitch-class use, also differed between the two groups: human players often favored certain tonal centers or voicings, while AI tended toward a more uniform distribution across keys and harmonies.</p>
<p>A Chi-square test found no significant association between feature distribution and generation platform (p &gt; 0.05), suggesting that differences are not platform-specific.</p>
</section>
<section id="assumption-checks-normality-variance-homogeneity-and-multicollinearity" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="assumption-checks-normality-variance-homogeneity-and-multicollinearity"><span class="header-section-number">3.5</span> Assumption Checks: Normality, Variance Homogeneity, and Multicollinearity</h2>
<p>Before running parametric statistical tests such as Welch’s <em>t</em>-tests and ANOVA, we checked whether each feature met the assumptions of normality and equal variances between AI and human groups. Shapiro–Wilk tests indicated that all features deviated from normality in at least one group (<em>p</em> &lt; 0.05), so we did not assume normally distributed data. We also ran Levene’s tests for homogeneity of variances, which showed that several key features, such as <code>spectral_bandwidth_mean</code>, <code>tempo_std</code>, and <code>mfcc_3_mean</code>, had significantly different variances between classes. These results reinforced our choice of Welch’s <em>t</em>-test, which is robust to both non-normality and unequal variances, for all hypothesis testing.</p>
<p>For the logistic regression model, we examined variance inflation factors (VIF) to assess multicollinearity among the ten final features. All VIF values were below the commonly used threshold of 5, indicating no problematic collinearity. This confirmed that each feature contributed unique information to the model and that coefficient estimates would remain stable and interpretable.</p>
</section>
<section id="feature-redundancy-check-and-final-selection-criteria" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="feature-redundancy-check-and-final-selection-criteria"><span class="header-section-number">3.6</span> Feature Redundancy Check and Final Selection Criteria</h2>
<p>To reduce multicollinearity and improve interpretability, we computed pairwise Pearson correlations (<a href="#fig-corr" class="quarto-xref">Figure&nbsp;4</a>) among all top-ranked features and enforced a threshold of |r| &lt; 0.7, meaning no two selected features could be too closely correlated. The only exception was <code>spectral_bandwidth_mean</code> and <code>spectral_centroid_mean</code>, which were moderately correlated (<em>r</em> ≈ 0.73) but retained due to their distinct acoustic interpretations: bandwidth reflects spread of energy across frequencies, while centroid captures perceived brightness.</p>
<div id="fig-corr" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/corrheatmap.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-corr-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Pairwise Pearson correlation heatmap for the ten final acoustic features.
</figcaption>
</figure>
</div>
<p><em>Note.</em> Shows all off-diagonal correlations below 0.7 (except the one justified exception), confirming non-redundancy of the selected feature set.</p>
<p>This filtering step ensured that our final subset of ten acoustic features was both non-redundant and consistently predictive.</p>
</section>
<section id="variance-inflation-factor-vif-analysis" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="variance-inflation-factor-vif-analysis"><span class="header-section-number">3.7</span> Variance Inflation Factor (VIF) Analysis</h2>
<p>To further confirm that our selected features were not excessively collinear, we calculated Variance Inflation Factors (VIF) for each feature in the final set. VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors. A VIF of 1 indicates no correlation with other variables, while values above 5–10 are generally considered problematic.</p>
<p>As shown in Table 2, all features had VIF values well below the threshold of 5, with most under 2, indicating minimal multicollinearity. This result supports our earlier Pearson correlation check (<a href="#fig-corr" class="quarto-xref">Figure&nbsp;4</a>) and provides additional assurance that each feature contributes unique information to the model without introducing instability into the coefficient estimates.</p>
<p><strong>Table 2. Variance Inflation Factors for Final Features</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Feature</th>
<th style="text-align: right;">VIF</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>spectral_bandwidth_mean</td>
<td style="text-align: right;">3.682</td>
</tr>
<tr class="even">
<td>spectral_centroid_mean</td>
<td style="text-align: right;">3.118</td>
</tr>
<tr class="odd">
<td>mfcc_3_mean</td>
<td style="text-align: right;">1.525</td>
</tr>
<tr class="even">
<td>tempo_std</td>
<td style="text-align: right;">1.291</td>
</tr>
<tr class="odd">
<td>onset_rate</td>
<td style="text-align: right;">1.261</td>
</tr>
<tr class="even">
<td>chroma_stft_mean</td>
<td style="text-align: right;">1.237</td>
</tr>
<tr class="odd">
<td>mfcc_5_mean</td>
<td style="text-align: right;">1.128</td>
</tr>
<tr class="even">
<td>tonnetz_3_mean</td>
<td style="text-align: right;">1.088</td>
</tr>
<tr class="odd">
<td>tonnetz_6_mean</td>
<td style="text-align: right;">1.081</td>
</tr>
<tr class="even">
<td>mfcc_delta2_13_mean</td>
<td style="text-align: right;">1.020</td>
</tr>
</tbody>
</table>
</section>
<section id="exploratory-visualization-with-selected-features" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="exploratory-visualization-with-selected-features"><span class="header-section-number">3.8</span> Exploratory Visualization with Selected Features</h2>
<p>To gain intuition about the separability of human-performed versus AI-generated piano excerpts in our selected feature space, we projected the ten features into two dimensions using t-distributed Stochastic Neighbor Embedding (t-SNE). As shown in <a href="#fig-tsne" class="quarto-xref">Figure&nbsp;5</a>, each point represents a 30-second clip, colored yellow for humans and purple for AI.</p>
<div id="fig-tsne" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tsne-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/t-sne.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tsne-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Two-dimensional t-SNE embedding of the ten selected acoustic features, with human-performed excerpts shown in yellow and AI-generated excerpts shown in purple.
</figcaption>
</figure>
</div>
<p><em>Note.</em> Human-performed excerpts are shown as yellow circles and AI-generated excerpts as purple circles. Pockets of yellow and purple illustrate regions where the acoustic signatures of human timing and timbre versus AI steadiness and spectral breadth tend to cluster.</p>
<p>While the two classes are not perfectly separated, reflecting overlap in certain acoustic traits, there are clear regions where human excerpts cluster, often marked by more expressive timing variations and narrower spectral spreads. AI-generated clips, in contrast, frequently group together in areas characterized by steadier tempi and broader spectral energy. Notably, outlier analysis revealed that most extreme cases in the AI class came from the Suno platform, which tended to produce pieces with unusually high spectral bandwidth and onset rates. These characteristics explain much of the tight clustering for Suno-generated excerpts, while other AI platforms showed more dispersion. Occasional overlaps between classes appear to stem from individual pieces whose acoustic profiles happen to align, rather than random noise, reinforcing the need for a supervised model to capture subtle but consistent distinctions across platforms.</p>
</section>
</section>
<section id="results" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Results</h1>
<section id="dataset-summary" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="dataset-summary"><span class="header-section-number">4.1</span> Dataset Summary</h2>
<p>The final dataset contained 722 thirty-second solo piano excerpts, evenly split between human-composed (<em>n</em> = 361) and AI-generated (<em>n</em> = 361) excerpts. Human recordings were drawn from public-domain libraries (FMA and Musopen) spanning classical, jazz, and contemporary styles. AI-generated examples came from four systems (AIVA, Udio, and two emergent models) to capture a range of algorithmic approaches. All source tracks exceeded 30 seconds, and metadata (platform, license, creator or generator name) was recorded for traceability.</p>
</section>
<section id="model-choice-and-pipeline" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="model-choice-and-pipeline"><span class="header-section-number">4.2</span> Model Choice and Pipeline</h2>
<p>Initial experiments with individual classifiers (logistic regression, random forest, gradient boosting) revealed trade-offs between interpretability and performance. We ultimately settled on a stacking model combining calibrated logistic regression, random forest (100 trees, <code>max_depth</code> = 5), gradient boosting, and naïve Bayes base learners. Pairwise interaction terms were added via second-degree polynomial features, and a logistic regression meta-learner was used to integrate their outputs. Stratified five-fold cross-validation guided hyperparameter selection and helped limit overfitting, while a final hold-out evaluation provided an unbiased assessment.</p>
</section>
<section id="model-evaluation" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="model-evaluation"><span class="header-section-number">4.3</span> Model Evaluation</h2>
<p><strong>Table 3. Hold-Out Test Results</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.821</td>
</tr>
<tr class="even">
<td>Precision</td>
<td>0.803</td>
</tr>
<tr class="odd">
<td>Recall</td>
<td>0.847</td>
</tr>
<tr class="even">
<td>F1 Score</td>
<td>0.824</td>
</tr>
<tr class="odd">
<td>ROC-AUC</td>
<td>0.924</td>
</tr>
</tbody>
</table>
<div id="fig-confmat" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confmat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/confusionmatrix.png" class="img-fluid figure-img" width="400">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confmat-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Confusion matrix for the stacking classifier on the 20% hold-out test set.
</figcaption>
</figure>
</div>
<p><em>Note.</em> The top-left and bottom-right cells indicate correctly classified samples, while the off-diagonals show misclassifications.</p>
</section>
<section id="model-performance-and-generalization" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="model-performance-and-generalization"><span class="header-section-number">4.4</span> Model Performance and Generalization</h2>
<p>On the training set, the logistic regression model achieved near-perfect performance (Accuracy = 0.995, Precision = 0.990, Recall = 1.000, F1 = 0.995, ROC–AUC = 1.000). Such high scores on the training data indicate that the model fits the training examples extremely well. However, the drop in performance on the held-out test set (Accuracy = 0.821, Precision = 0.803, Recall = 0.847, F1 = 0.824, ROC–AUC = 0.924) suggests some overfitting. The model memorizes patterns in the training data that do not perfectly transfer to unseen examples.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Training</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.995</td>
<td>0.821</td>
</tr>
<tr class="even">
<td>Precision</td>
<td>0.990</td>
<td>0.803</td>
</tr>
<tr class="odd">
<td>Recall</td>
<td>1.000</td>
<td>0.847</td>
</tr>
<tr class="even">
<td>F1 Score</td>
<td>0.995</td>
<td>0.824</td>
</tr>
<tr class="odd">
<td>ROC-AUC</td>
<td>1.000</td>
<td>0.924</td>
</tr>
</tbody>
</table>
<p>Even so, test-set results remain strong, particularly the ROC–AUC of 0.924, which reflects the model’s ability to discriminate between AI and human excerpts across all classification thresholds. The balance between precision and recall also indicates that the model performs well in both identifying AI-generated music and avoiding false positives.</p>
<p>The final stacking ensemble achieved 82.1% accuracy on the held-out test set, indicating that over four out of five excerpts were correctly classified as human or AI-generated. Precision was 0.803, meaning that when the model predicted an excerpt was AI-generated, it was correct about 80% of the time. Recall reached 0.847, showing that the model successfully identified roughly 85% of all AI-generated excerpts. The F1 score of 0.824 balances these precision and recall values, reflecting strong overall classification performance. The ROC-AUC of 0.924 indicates that, across all possible decision thresholds, the model ranks a randomly chosen AI excerpt higher than a randomly chosen human excerpt about 92.4% of the time, demonstrating excellent separability between the two classes.</p>
<div id="fig-roc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-roc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/ROCcurve.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-roc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Receiver operating characteristic (ROC) curve for the final stacking classifier, with AUC = 0.924 indicating strong discrimination between classes.
</figcaption>
</figure>
</div>
<p><em>Note.</em> The dotted diagonal line represents random guessing, and the model’s AUC = 0.924 indicates strong discrimination between human and AI excerpts.</p>
<p>Both classes had the same F1 score (0.82), and ROC-AUC was particularly informative as it evaluates ranking performance independently of any threshold.</p>
</section>
<section id="error-analysis" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="error-analysis"><span class="header-section-number">4.5</span> Error Analysis</h2>
<p>To explore where the model was less certain, we reviewed the five most extreme misclassifications in each category. <a href="#fig-tsne_errors" class="quarto-xref">Figure&nbsp;8</a> shows the instances where the classification resulted in an AI-generated excerpt being incorrectly labeled as a human-composed excerpt (false positive) and where a human-composed excerpt was incorrectly labeled as AI-generated (false negative).</p>
<div id="fig-tsne_errors" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tsne_errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Visualizations/erroranalysis.png" class="img-fluid figure-img" width="600">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tsne_errors-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: t-SNE projection of test excerpts highlighting misclassified samples: red crosses are false positives, blue crosses are false negatives, and faint gray points are correctly classified samples.
</figcaption>
</figure>
</div>
<p><em>Note.</em> Red crosses indicate false positives (human excerpts predicted as AI), blue crosses indicate false negatives (AI excerpts predicted as human), and faint gray points show correctly classified samples.</p>
<p>Some AI excerpts had a bright, wide sound but fewer timing changes, suggesting the model sometimes relied more on rhythm than tone. A few human excerpts kept an unusually steady tempo or had a balanced mix of notes, making them sound more like AI pieces. Overall, the stacking model was good at telling human and AI piano apart, but certain playing styles made the line between them less clear.</p>
<p><strong>Platform effects.</strong> A chi-square test found no significant association between platform and label prevalence (<em>p</em> &gt; 0.05), but Suno clips showed visually tight clusters and contributed many extremes in spectral bandwidth and onset rate in the outlier screen. This suggests stylistic/production tendencies can influence where clips appear in feature space even when label balance across platforms is not biased.</p>
</section>
</section>
<section id="discussion" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Discussion</h1>
<p>In this work, we developed a transparent, feature-based pipeline to distinguish 30-second solo piano excerpts as human-composed or AI-generated. By starting with a balanced dataset of 722 excerpts and extracting 55 acoustic descriptors, we used statistical tests and model-based rankings to hone in on a non-redundant set of ten features. Our enhanced stacking ensemble, combining calibrated logistic regression, random forest, gradient boosting, and naïve Bayes with interaction terms, achieved 82.1 % accuracy (0.924 ROC-AUC) on held-out data. While these results are encouraging, our primary goal is to illuminate which acoustic cues reliably signal “humanness” in piano performance.</p>
<section id="acoustic-cues-of-humanness" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="acoustic-cues-of-humanness"><span class="header-section-number">5.1</span> Acoustic Cues of “Humanness”</h2>
<p>Three main features stood out as the most useful for telling human and AI piano apart. Spectral bandwidth measures how sound energy is spread across low and high pitches. Human pianists often create a balance between brightness and warmth, while AI pieces sometimes sound either too bright or too flat. Tempo variability tracks how much the speed changes during a performance. Human players naturally slow down or speed up slightly for expression, while AI pieces tend to keep a steady beat. Chroma balance looks at how evenly different notes and chords are used. Humans often vary chord shapes and emphasize certain tones in a way that feels more natural and emotional, while AI outputs can sound more uniform. Together, these features capture differences in tone, rhythm, and harmony that often make human performances feel more expressive.</p>
</section>
<section id="error-modes-and-model-boundaries" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="error-modes-and-model-boundaries"><span class="header-section-number">5.2</span> Error Modes and Model Boundaries</h2>
<p>We found two main ways the model made mistakes. Some AI pieces, even though they were computer-generated, had a loose, human-like timing pattern (similar to rubato) but also very bright, wide-ranging tones. These fooled the model into thinking they were human, which suggests the model sometimes focuses more on timing than tone. On the other hand, some human performances were so steady in tempo and so average in harmonic balance that they came across as mechanical, leading the model to label them as AI. In short, very “human-like” AI playing and very “machine-like” human playing were the trouble spots. These cases suggest the model might do better if it could also pick up on other cues, like how the sustain pedal is used or finer details of timing changes.</p>
</section>
<section id="data-ethics" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="data-ethics"><span class="header-section-number">5.3</span> Data Ethics</h2>
<p>This project has several limitations and ethical considerations that should be acknowledged. Our human recordings were drawn from public-domain sources (FMA, Musopen) under clear licensing terms, but these archives do not fully capture the range of piano styles, traditions, or performer backgrounds. As a result, the model may reflect biases if applied to music outside this narrow scope.</p>
<p>There is also a risk of misclassification. Incorrectly labeling a human performance as AI-generated could unfairly affect an artist’s reputation, while missing AI-generated content could allow for misuse or misattribution. For this reason, any automated predictions should be treated as preliminary, ideally accompanied by confidence scores and, when possible, reviewed by human listeners.</p>
<p>Publishing the specific acoustic patterns that distinguish our dataset’s human and AI recordings could enable future AI systems to imitate these patterns more effectively, reducing the effectiveness of static detectors. Detection tools will likely need ongoing updates, external review, and collaborative oversight to remain reliable.</p>
<p>Finally, some commercial AI music platforms, including Suno and AIVA, have been criticized for producing works that closely resemble human artists’ styles. If these systems are trained on copyrighted material, their outputs could raise questions around intellectual property and fair use.</p>
</section>
<section id="directions-for-future-work" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="directions-for-future-work"><span class="header-section-number">5.4</span> Directions for Future Work</h2>
<p>To broaden applicability, future studies should incorporate MIDI/symbolic data for note-level analysis (voice-leading, chord progressions) alongside audio features. Enhancing temporal modeling with metrics like onset-interval autocorrelation or beat-synchronous energy flows could improve sensitivity to expressive phrasing. Hybrid approaches that integrate interpretable features with lightweight neural embeddings (e.g., timbral autoencoders) may capture subtler spectral artifacts. Finally, evaluating the pipeline on unseen AI systems and “in-the-wild” streaming data, using nested cross-validation to guard against overfitting, will be critical for real-world deployment.</p>
<p>By clearly identifying which acoustic characteristics drive classification and acknowledging both technical and ethical challenges, we offer a replicable foundation for ongoing work in automated AI-music detection, helping to support listener trust and fair recognition of human artistry.</p>
<hr>
</section>
</section>
<section id="appendix" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Appendix:</h1>
<p><strong>Table A1.</strong> Detailed feature values for the top five false positives (human excerpts mislabeled as AI) in the hold-out test set. Each row lists the excerpt index, key acoustic feature measurements, and the true versus predicted labels.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 18%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Index</th>
<th>spectral_bandwidth_mean</th>
<th>tempo_std</th>
<th>chroma_stft_mean</th>
<th>onset_rate</th>
<th>True Label</th>
<th>Pred Label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>924.23</td>
<td>50.03</td>
<td>0.282</td>
<td>3.47</td>
<td>Human</td>
<td>AI</td>
</tr>
<tr class="even">
<td>8</td>
<td>648.68</td>
<td>240.22</td>
<td>0.272</td>
<td>3.40</td>
<td>Human</td>
<td>AI</td>
</tr>
<tr class="odd">
<td>12</td>
<td>616.12</td>
<td>249.10</td>
<td>0.281</td>
<td>3.67</td>
<td>Human</td>
<td>AI</td>
</tr>
<tr class="even">
<td>35</td>
<td>808.12</td>
<td>174.78</td>
<td>0.319</td>
<td>7.80</td>
<td>Human</td>
<td>AI</td>
</tr>
<tr class="odd">
<td>48</td>
<td>739.49</td>
<td>271.98</td>
<td>0.240</td>
<td>2.57</td>
<td>Human</td>
<td>AI</td>
</tr>
</tbody>
</table>
<p><strong>Table A2.</strong> Detailed feature values for the top five false negatives (AI excerpts mislabeled as human) in the hold-out test set. Each row lists the excerpt index, key acoustic feature measurements, and the true versus predicted labels.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 25%">
<col style="width: 11%">
<col style="width: 18%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Index</th>
<th>spectral_bandwidth_mean</th>
<th>tempo_std</th>
<th>chroma_stft_mean</th>
<th>onset_rate</th>
<th>True Label</th>
<th>Pred Label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>749.85</td>
<td>209.21</td>
<td>0.269</td>
<td>6.43</td>
<td>AI</td>
<td>Human</td>
</tr>
<tr class="even">
<td>37</td>
<td>1330.79</td>
<td>254.11</td>
<td>0.248</td>
<td>1.87</td>
<td>AI</td>
<td>Human</td>
</tr>
<tr class="odd">
<td>67</td>
<td>2676.89</td>
<td>277.61</td>
<td>0.218</td>
<td>1.77</td>
<td>AI</td>
<td>Human</td>
</tr>
<tr class="even">
<td>68</td>
<td>549.07</td>
<td>67.58</td>
<td>0.277</td>
<td>1.77</td>
<td>AI</td>
<td>Human</td>
</tr>
<tr class="odd">
<td>79</td>
<td>737.93</td>
<td>229.27</td>
<td>0.225</td>
<td>2.53</td>
<td>AI</td>
<td>Human</td>
</tr>
</tbody>
</table>
<p><strong>Table A3.</strong> Summary of univariate hypothesis tests for the top ten features. Columns include Welch’s t-statistic and p-value (unequal variances), absolute point-biserial correlation (|r_pb|), one-way ANOVA F-statistic and p-value, Tukey HSD 95% confidence intervals for group mean differences, and Cohen’s d effect sizes.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 6%">
<col style="width: 12%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>t-stat</th>
<th>p-value (Welch)</th>
<th>|r_pb|</th>
<th>F-stat</th>
<th>p-value (ANOVA)</th>
<th>Tukey CI lower</th>
<th>Tukey CI upper</th>
<th>Cohen’s d</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>spectral_bandwidth_mean</td>
<td>9.032</td>
<td>&lt; 0.0001</td>
<td>0.319</td>
<td>81.574</td>
<td>&lt; 0.0001</td>
<td>–424.63</td>
<td>–272.99</td>
<td>0.672</td>
</tr>
<tr class="even">
<td>tempo_std</td>
<td>7.521</td>
<td>&lt; 0.0001</td>
<td>0.270</td>
<td>56.563</td>
<td>&lt; 0.0001</td>
<td>–69.29</td>
<td>–40.61</td>
<td>0.560</td>
</tr>
<tr class="odd">
<td>chroma_stft_mean</td>
<td>7.165</td>
<td>&lt; 0.0001</td>
<td>0.258</td>
<td>51.343</td>
<td>&lt; 0.0001</td>
<td>–0.03</td>
<td>–0.02</td>
<td>0.533</td>
</tr>
<tr class="even">
<td>onset_rate</td>
<td>–6.280</td>
<td>&lt; 0.0001</td>
<td>0.228</td>
<td>39.442</td>
<td>&lt; 0.0001</td>
<td>0.47</td>
<td>0.89</td>
<td>0.467</td>
</tr>
<tr class="odd">
<td>mfcc_3_mean</td>
<td>4.925</td>
<td>&lt; 0.0001</td>
<td>0.181</td>
<td>24.255</td>
<td>&lt; 0.0001</td>
<td>–9.32</td>
<td>–4.01</td>
<td>0.367</td>
</tr>
<tr class="even">
<td>spectral_centroid_mean</td>
<td>4.721</td>
<td>&lt; 0.0001</td>
<td>0.173</td>
<td>22.285</td>
<td>&lt; 0.0001</td>
<td>–144.58</td>
<td>–59.65</td>
<td>0.351</td>
</tr>
<tr class="odd">
<td>mfcc_5_mean</td>
<td>–3.664</td>
<td>0.0003</td>
<td>0.273</td>
<td>13.427</td>
<td>0.0003</td>
<td>0.87</td>
<td>2.87</td>
<td>0.273</td>
</tr>
<tr class="even">
<td>tonnetz_3_mean</td>
<td>–3.629</td>
<td>0.0003</td>
<td>0.270</td>
<td>13.169</td>
<td>0.0003</td>
<td>0.02</td>
<td>0.06</td>
<td>0.270</td>
</tr>
<tr class="odd">
<td>tonnetz_6_mean</td>
<td>3.454</td>
<td>0.0006</td>
<td>0.128</td>
<td>11.932</td>
<td>0.0006</td>
<td>–0.02</td>
<td>–0.00</td>
<td>0.257</td>
</tr>
<tr class="even">
<td>mfcc_delta2_13_mean</td>
<td>–3.173</td>
<td>0.0016</td>
<td>0.117</td>
<td>10.067</td>
<td>0.0016</td>
<td>0.00</td>
<td>0.00</td>
<td>0.236</td>
</tr>
</tbody>
</table>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<p>Afchar, D., Issam, W., &amp; Prévost, C. (2025). Detecting generative AI in music: First public AI-music detector and its challenges. <em>arXiv</em>. <a href="https://arxiv.org/html/2501.10111v1" class="uri">https://arxiv.org/html/2501.10111v1</a></p>
<p>Barlow, G. (2025, July 3). Apple and Spotify are sleepwalking into an AI music crisis – and The Velvet Sundown mess shows they need to act fast. <em>TechRadar</em>. <a href="https://www.techradar.com/computing/artificial-intelligence/apple-and-spotify-are-sleepwalking-into-an-ai-music-crisis-and-the-velvet-sundown-mess-shows-they-need-to-act-fast?utm_source=chatgpt.com" class="uri">https://www.techradar.com/computing/artificial-intelligence/apple-and-spotify-are-sleepwalking-into-an-ai-music-crisis-and-the-velvet-sundown-mess-shows-they-need-to-act-fast?utm_source=chatgpt.com</a></p>
<p>Beck, E. (2025, July 8). A ’60s-flavored band blew up on Spotify. They’re AI. <em>The Washington Post</em>. <a href="https://www.washingtonpost.com/entertainment/music/2025/07/08/velvet-sundown-artificial-intelligence-spotify/" class="uri">https://www.washingtonpost.com/entertainment/music/2025/07/08/velvet-sundown-artificial-intelligence-spotify/</a></p>
<p>Dervakos, E., Filandrianos, G., &amp; Stamou, G. (2021). Heuristics for evaluation of AI-generated music [Poster]. <em>ICPR 2021</em>. <a href="https://ailb-web.ing.unimore.it/icpr/media/posters/11986.pdf" class="uri">https://ailb-web.ing.unimore.it/icpr/media/posters/11986.pdf</a></p>
<p>Flexer, A., Schnitzer, D., &amp; Widmer, G. (2006). Combination of spectral and rhythmic similarity feature spaces for music classification (OeFAI TR-2006-09). <em>Austrian Research Institute for Artificial Intelligence</em>. <a href="https://ofai.at/papers/oefai-tr-2006-09.pdf" class="uri">https://ofai.at/papers/oefai-tr-2006-09.pdf</a></p>
<p>Misra, S. (2022). An AI model to differentiate AI-generated music from human-composed music [Independent Project Mentorship]. <a href="https://independent-project-mentorship.netlify.app/assets/pdfs/2ededfe6e527c0b86f86b57d56647c1400ba9a27.pdf" class="uri">https://independent-project-mentorship.netlify.app/assets/pdfs/2ededfe6e527c0b86f86b57d56647c1400ba9a27.pdf</a></p>
<p>MusicAlly. (2025, July 8). Key quotes from Michael Nash’s keynote at the AI for Good Global Summit. <a href="https://musically.com/2025/07/08/umgs-michael-nash-ai-can-be-fundamental-to-the-future-of-music/" class="uri">https://musically.com/2025/07/08/umgs-michael-nash-ai-can-be-fundamental-to-the-future-of-music/</a></p>
<p>Noh, M., &amp; Kim, C. (2025). Harmony and personality: Analyzing connections between AI-generated music preference and personal traits. <em>Nursing and Healthcare Science Journal</em>. <a href="https://nhsjs.com/2025/harmony-and-personality-analyzing-connections-between-ai-generated-music-preference-and-personal-traits/" class="uri">https://nhsjs.com/2025/harmony-and-personality-analyzing-connections-between-ai-generated-music-preference-and-personal-traits/</a></p>
<p>Press-Reynolds, E. (2025, July 2). How AI wreaked havoc on the lo-fi beat scene. <em>Pitchfork</em>. <a href="https://pitchfork.com/thepitch/how-ai-wreaked-havoc-on-the-lo-fi-beat-scene/?utm_source=chatgpt.com" class="uri">https://pitchfork.com/thepitch/how-ai-wreaked-havoc-on-the-lo-fi-beat-scene/?utm_source=chatgpt.com</a></p>
<p>Reuters. (2025, July 8). Resurgence of music securitization attracts investors, but emerging risks like AI remain considerations. <em>Reuters</em>. <a href="https://www.reuters.com/legal/legalindustry/resurgence-music-securitization-issuer-investor-appeal-data-driven-era-2025-07-08/" class="uri">https://www.reuters.com/legal/legalindustry/resurgence-music-securitization-issuer-investor-appeal-data-driven-era-2025-07-08/</a></p>
<p>Sarmento, P., Loth, J., &amp; Barthet, M. (2024). Between the AI and Me: Analysing listeners’ perspectives on AI- and human-composed progressive metal music. <em>arXiv</em>. <a href="https://arxiv.org/abs/2407.21615" class="uri">https://arxiv.org/abs/2407.21615</a></p>
<p>Schneider, L. (2025, May 19). AI music is more common – and harder to catch – than ever. <em>Scienceline</em>. <a href="https://scienceline.org/2025/05/ai-music-is-more-common-and-harder-to-catch-than-ever/" class="uri">https://scienceline.org/2025/05/ai-music-is-more-common-and-harder-to-catch-than-ever/</a></p>
<p>University of York. (2023). AI-generated music inferior to human-composed music. <em>University of York</em>. <a href="https://www.york.ac.uk/news-and-events/news/2023/research/ai-generated-music-inferior-to-human-composed/" class="uri">https://www.york.ac.uk/news-and-events/news/2023/research/ai-generated-music-inferior-to-human-composed/</a></p>
<p>Wikipedia contributors. (n.d.). Music and artificial intelligence. In <em>Wikipedia, The Free Encyclopedia</em>. Retrieved July 28, 2025, from <a href="https://en.wikipedia.org/wiki/Music_and_artificial_intelligence" class="uri">https://en.wikipedia.org/wiki/Music_and_artificial_intelligence</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>