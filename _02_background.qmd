# Background
Differentiating AI-generated from human-composed music is a growing focus of research, spanning both listener perception and computational detection methods. Recent perceptual studies (Collins et al., 2023; Sarmento et al., 2024) have found that listeners generally struggle to distinguish AI-generated music from human-performed pieces, yet tend to express a clear preference for human performances.

Computational detection techniques have achieved strong results. For example, deep neural networks trained directly on audio spectrograms (Afchar et al., 2025; Vernet et al., 2025) and transformer-based methods (Independent Project, 2022) have reported accuracies above 95%. However, these approaches typically lack transparency due to their complexity and reliance on abstract learned representations.

Alternatively, simpler acoustic features derived directly from audio signals offer greater interpretability. Features such as spectral centroids and Mel-Frequency Cepstral Coefficients (MFCCs) effectively describe differences in timbre between AI-generated and human-played music (Dervakos et al., 2021). Temporal descriptors, including tempo variability and onset rate, reflect human expressive timing patterns and subtle rhythmic fluctuations (Flexer et al., 2006; Sarmento et al., 2024). Harmonic descriptors (e.g., Tonnetz features, chroma balance) further characterize structural differences in tonal and harmonic organization.

This current project aims to contribute to the current research by focusing on the differences in acoustic features between AI-generated and human-composed 30 second solo piano pieces, and to create a classification model that is able to reliably predict whether or not a 30 second solo piano piece is AI-generated or composed by a human.
